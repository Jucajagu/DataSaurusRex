@article{LeCun1989,
   abstract = {<p>The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.</p>},
   author = {Y. LeCun and B. Boser and J. S. Denker and D. Henderson and R. E. Howard and W. Hubbard and L. D. Jackel},
   doi = {10.1162/neco.1989.1.4.541},
   issn = {0899-7667},
   issue = {4},
   journal = {Neural Computation},
   month = {12},
   pages = {541-551},
   title = {Backpropagation Applied to Handwritten Zip Code Recognition},
   volume = {1},
   url = {https://direct.mit.edu/neco/article/1/4/541-551/5515},
   year = {1989},
}
@article{Raut2023,
   author = {R. Raut},
   issn = {2147-6799},
   journal = {International Journal of Intelligent Systems and Applications in Engineering},
   pages = {593-605},
   title = {An Analytical Approach on Various Deep Learning Models for Image Classification},
   volume = {11},
   year = {2023},
}
@article{Zhang2024,
   author = {Jingyi Zhang and Jiaxing Huang and Sheng Jin and Shijian Lu},
   doi = {10.1109/TPAMI.2024.3369699},
   issn = {0162-8828},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   pages = {1-20},
   title = {Vision-Language Models for Vision Tasks: A Survey},
   url = {https://ieeexplore.ieee.org/document/10445007/},
   year = {2024},
}
@article{Ponnusamy2017,
   author = {R Ponnusamy and S Sathyamoorthy and K Manikandan},
   doi = {10.23883/IJRTER.2017.3033.XTS7Z},
   issn = {24551457},
   issue = {3},
   journal = {International Journal of Recent Trends in Engineering and Research},
   month = {3},
   pages = {1-5},
   title = {A Review of Image Classification Approaches and Techniques},
   volume = {3},
   url = {http://www.ijrter.com/papers/volume-3/issue-3/a-review-of-image-classification-approaches-and-techniques.pdf},
   year = {2017},
}
@article{Sharma2024,
   author = {Rishabh Sharma and Vinay Kukreja},
   doi = {10.1016/j.engappai.2023.107715},
   issn = {09521976},
   journal = {Engineering Applications of Artificial Intelligence},
   month = {5},
   pages = {107715},
   title = {Image segmentation, classification and recognition methods for comics: A decade systematic literature review},
   volume = {131},
   year = {2024},
}
@article{Cireşan2011,
   abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.},
   author = {Dan C. Cireşan and Ueli Meier and Jonathan Masci and Luca M. Gambardella and Jürgen Schmidhuber},
   month = {2},
   title = {High-Performance Neural Networks for Visual Object Classification},
   url = {http://arxiv.org/abs/1102.0183},
   year = {2011},
}
@article{Dumoulin2016,
   abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
   author = {Vincent Dumoulin and Francesco Visin},
   month = {3},
   title = {A guide to convolution arithmetic for deep learning},
   url = {http://arxiv.org/abs/1603.07285},
   year = {2016},
}
@inbook{Zeiler2014,
   abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets. © 2014 Springer International Publishing.},
   author = {Matthew D. Zeiler and Rob Fergus},
   doi = {10.1007/978-3-319-10590-1_53},
   isbn = {9783319105895},
   issn = {16113349},
   issue = {PART 1},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {818-833},
   publisher = {Springer Verlag},
   title = {Visualizing and Understanding Convolutional Networks},
   volume = {8689 LNCS},
   url = {http://link.springer.com/10.1007/978-3-319-10590-1_53},
   year = {2014},
}
@article{Chen2021,
   abstract = {<p>Image classification has always been a hot research direction in the world, and the emergence of deep learning has promoted the development of this field. Convolutional neural networks (CNNs) have gradually become the mainstream algorithm for image classification since 2012, and the CNN architecture applied to other visual recognition tasks (such as object detection, object localization, and semantic segmentation) is generally derived from the network architecture in image classification. In the wake of these successes, CNN-based methods have emerged in remote sensing image scene classification and achieved advanced classification accuracy. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art (SOAT) network architectures. Along the way, we analyze (1) the basic structure of artificial neural networks (ANNs) and the basic network layers of CNNs, (2) the classic predecessor network models, (3) the recent SOAT network algorithms, (4) comprehensive comparison of various image classification methods mentioned in this article. Finally, we have also summarized the main analysis and discussion in this article, as well as introduce some of the current trends.</p>},
   author = {Leiyu Chen and Shaobo Li and Qiang Bai and Jing Yang and Sanlong Jiang and Yanming Miao},
   doi = {10.3390/rs13224712},
   issn = {2072-4292},
   issue = {22},
   journal = {Remote Sensing},
   keywords = {convolutional neural networks,deep learning,image classification},
   month = {11},
   pages = {4712},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Review of Image Classification Algorithms Based on Convolutional Neural Networks},
   volume = {13},
   url = {https://www.mdpi.com/2072-4292/13/22/4712},
   year = {2021},
}
@article{Sharma2018,
   abstract = {This paper presents an empirical analysis of theperformance of popular convolutional neural networks (CNNs) for identifying objects in real time video feeds. The most popular convolution neural networks for object detection and object category classification from images are Alex Nets, GoogLeNet, and ResNet50. A variety of image data sets are available to test the performance of different types of CNN's. The commonly found benchmark datasets for evaluating the performance of a convolutional neural network are anImageNet dataset, and CIFAR10, CIFAR100, and MNIST image data sets. This study focuses on analyzing the performance of three popular networks: Alex Net, GoogLeNet, and ResNet50. We have taken three most popular data sets ImageNet, CIFAR10, and CIFAR100 for our study, since, testing the performance of a network on a single data set does not reveal its true capability and limitations. It must be noted that videos are not used as a training dataset, they are used as testing datasets. Our analysis shows that GoogLeNet and ResNet50 are able to recognize objects with better precision compared to Alex Net. Moreover, theperformance of trained CNN's vary substantially across different categories of objects and we, therefore, will discuss the possible reasons for this.},
   author = {Neha Sharma and Vibhor Jain and Anju Mishra},
   doi = {10.1016/j.procs.2018.05.198},
   issn = {18770509},
   journal = {Procedia Computer Science},
   keywords = {CNN,Deep Learning,Neural network,Object classification,Object detection},
   month = {1},
   pages = {377-384},
   publisher = {Elsevier},
   title = {An Analysis Of Convolutional Neural Networks For Image Classification},
   volume = {132},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918309335},
   year = {2018},
}
@misc{Young1995,
   author = {I T Young and J J Gerbrands and L J Van Vliet},
   title = {Fundamentals of Image Processing},
   year = {1995},
}

@article{Krizhevsky2012,
  author          = {A Krizhevsky and H Sutskever and G Hinton},
  journal         = {Advances in neural information processing system},
  number          = {},
  title           = {ImageNet Classification with Deep Convolutional Neural Networks},
  volume          = {25},
  year            = {2012}
}